# Faith and Fate: Limits of Transformers on Compositionality

## 論文について (掲載ジャーナルなど)
- [Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, Yejin Choi](https://arxiv.org/abs/2305.18654)

## 概要
- 近年、LLMは大きな注目を集め、特に複雑な多段階の推論を必要とするタスクで高性能を発揮することがよく知られている
- しかしながら、驚くほど些細な問題で失敗する場合もある
  - 「なぜ失敗するのか」について検討した
- 本論文は、以下の4つのコントリビューションがある
    - LLMを解明する試みとして、3つの代表的な compositional tasks においてモデルの限界を調査
        - 桁の多い数字の掛け算、ロジックグリッドパズル、古典的な動的計画問題
        - compositional tasks =「問題をサブステップに分解し、それらのステップの結果を合成して正解を導く」ような問題
    - 複雑さのレベルを体系的に定量化するため、compositional tasksを計算グラフとして定式化
    - Transformers の compositional tasksを解く方法の洞察
      - 体系的な問題解決スキルを獲得しているわけではない
      - 多段階の構成的推論を線形化された部分グラフのマッチングに還元することで、compositional tasksを解いている（ようだ）
    - タスクの複雑性が増加するにしたがい、Transformersのパフォーマンスが急速に低下する実験結果

## 問題設定と解決したこと
- 問題設定
  - LLM の compositional tasks を解く能力の限界を知りたい
    - LLMは、いろんなタスクで高性能を発揮する一方、簡単な $3\times3$ の計算の正解率は55%~59% ←なんで？？
- 仮説
    1. Transformers は多段階の構成的推論を線形化されたパスマッチングに還元することにより、compositional tasksを解く
       - パターンマッチングによるショートカット学習は、トレーニング中に類似の構成パターンが利用可能な場合、高速な正解を得ることができる
       - 一般的でない例や複雑な例に対するロバストな汎化はできない
    2. エラーの伝播により、Transformers は新しいパターンの高複雑度の compositional tasks を解くことは本質的な限界がある
       - 計算プロセスの初期段階でのエラーが後続のステップにおける複合エラーにつながり、モデルが正解を見つけることを妨げるだろう

- 解決したこと
  - compositional tasks の複雑さのレベルを体系的に定量化する方法を提案
  - LLMの推論力を調べた
    - LLMの推論力の傾向
      - タスクに特化したデータを用いた学習は、ドメイン内のインスタンスや低い構成複雑度のもとではほぼ完璧な性能を示す
      - この領域外のインスタンスでは大きく失敗する
    - LLMのドメインの領域内外でのギャップにより以下のことが示唆される
      - **人間のような推論ステップを促したり訓練したとしても、入出力のシーケンスを尤もらしく繋げる方法（maximum likelihood training）からは、体系的な問題解決能力が生まれないのではないか**
  - LLMの失敗するケースの分析の結果
    - **モデルは単一ステップの演算を記憶することはできるが、それらを正しい推論パスに構成することはできず、深い全体的な課題理解ではなく、浅い暗記学習に基づいて予測を行うことがほとんど！**

## 何をどう使ったのか
- 人間の問題解決能力はグラフ構造として概念化することができる
  - 各頂点は部分解を表し、辺はこれらの解を修正するために適用できる演算子を表す
  - 以下の図1にしめすように、計算グラフと対応する評価方法を使用して、Transformers の推論力を評価する
    - <img src="picture/Faith and Fate; Limits of Transformers on Compositionality Figure 1.png" alt="Faith and Fate; Limits of Transformers on Compositionality Figure 1" style="zoom: 33%;" />
- 推論を計算グラフのとして表現することで、タスクの複雑さを様々な視点で測定することができる
  - 推論の深さ (reasoning depth)
    - タスクを解決するために必要なマルチホップ推論の最大値の（代わりに示す）指標
  - 推論の幅 (reasoning width)
    - 計算中に並列して維持する変数の最大値の指標
  - グラフの平均並列度 (average parallelism)
    - 「推論の深さ」と「推論の幅」の比
    - グラフを通して計算の平均幅（保持すべき変数の数の平均）を算出する指標
  - 相対情報利得
    - 計算グラフの任意のノードの、「そのノードより前のノードの集合」に対する影響力を分析できる指標
    - この指標をいれるモチベーション
      - モデルの性能を評価するとき、全体的に不正解でも部分的に正解であるケースがある
      - 部分的な成功におけるモデルの戦略を理解したいので、モデルが認識しやすい表層パターンを予測したい
        - [意見] モデルが認識しやすい表層パターン？？？？？
- モデルに解かせる compositional tasks の定義
  1. 乗算 (Multiplication)
     - 桁の多い数同士の乗算は、手続き的ルールに基づいて数値記号の演算を実行する必要がある
     - 計算グラフを構築する際に、long-form multiplication アルゴリズムを用いる
  2. アインシュタインのパズル (Einstein's Puzzle)
     - 制約充足問題を解くベンチマークとして使われる有名な論理パズル
     - 異なる形の家があり、それぞれの家に住む人の出身国・好みの飲み物・好みのタバコ・飼っているペットなどの説明文が箇条書きでたくさん並んでる。たくさんの説明文を頼りに、それぞれの家に住んでいる家族とその好みを当てるパズル
  3. 動的計画法問題
     - 複雑な問題をより単純な部分問題に再帰的に分解して解く動的計画法を使う問題
       - 整数の並びが与えられたとき、その並びの中の２つの数が元の並びの中で隣接しないような最大の和を持つ並びを見つける（みたいな）

## 主張の有効性の検証方法
- 実験設定
  - モデル：GPT3, ChatGPT, GPT4
  - 評価時の学習：ゼロショット、few ショット、ファインシューニング
  - モデルの回答形式：質問-答え形式、質問-スクラッチパッド（←計算グラフを言語化したもの）形式

- ゼロショット時の結果
  - 問題の複雑さが増すにつれ、ほぼ完璧に回答→ほぼ不正解へと大幅に悪化する
    - <img src="picture/Faith and Fate; Limits of Transformers on Compositionality Figure 2.png" alt="Faith and Fate; Limits of Transformers on Compositionality Figure 2" style="zoom: 33%;" />
      - 問題の複雑さは、問題サイズ or 平均並列度で測った
  - ゼロショット時の結果からわかること
    - **compositional tasksを解くための基本的な操作の組み合わせ方をモデルに教えるには、事前学習では不十分**

- 質問-回答形式の学習による Transformers の限界
  - 疑問：
    - ゼロショットの結果が悪い。なぜか？
      - モデルの性能に限界があるのは、事前学習時にタスクに特化したデータがなかったかもしれない
  - 解決案：
    - ファインチューニングしてみよう
      - 乗算、アインシュタインのパズル、動的計画問題の小さい問題（最大問題サイズ3）とその回答をいれまくってGPT-3をチューニング
  - 結果：
    - 見たことある範囲の複雑さの問題は解けるようになったが、領域外の問題では性能が急激に低下
    - <img src="picture/Faith and Fate; Limits of Transformers on Compositionality Figure 4.png" alt="Faith and Fate; Limits of Transformers on Compositionality Figure 4" style="zoom: 33%;" />
  - 結論：
    - **系統的な問題解決能力は、タスク固有のデータに対する徹底的な訓練によって出現するものではない**ことが示唆された
    -

- 質問-スクラッチパッド形式の学習による Transformers の限界
  - 疑問：
    - 質問-回答形式がモデルにマッチしていないんかも？
  - 解決案：
    - 必要な計算操作をスクラッチパッドを使って明示的にモデルに教えられるか試してみた
  - 結果：
    - 質問-回答形式とほぼ同様に、見たことある複雑さの問題は解けるが、領域外はダメダメ
      - より広い、より深い計算グラフへの一般化では完全に失敗
    - 図4b参照

  - 結論：
    - スクラッチパッド形式を用いて、計算ステップに関するガイダンスを用いて直接学習した場合でも、領域外の問題は解けるようにはならない
    - **Transformer が問題に逐次的に取り組まざるを得ないという自己回帰的な特性が、モデルにステップバイステップの解を生成するように指示しても解決できない基本的な課題を提示している**ことを示唆している
      - モデルはタスクの厳密なグローバルな理解せずに、次の単語を生成する貪欲なプロセスに依存して予測を行っている
        - 見たことあるものの領域外の複雑さの問題を解くためには、「逐次的に次の単語を予測するTransformer」とは異なる、「タスクを厳密にグローバルに理解する特性を持つモデル」が必要なのではないかということを示唆

- Transformers の成功と失敗の深掘り
  - 情報利得によるTransformersの部分的な成功の調査
    - モチベーション：
      - Transformersは全体的な回答が不正解であっても、部分的に正しい答えを予測することがある
      - 相対情報利得を用いれば、モデルが学習しそうな表層パターンを予測し、経験的に表層パターンを対比することができそう
        - 乗算の場合
        - 相対情報利得により、出力の最初の桁が各入力数値の最初の桁と高い相関があることがわかる
          - →この偽のパターンはモデルによって学習される可能性がある
    - 実験：
      - Appendix C.1を参照
    - 結果：
      - Transformers は学習中にそのような相関関係を認識し、テストでは厳密なマルチホップ推論を経ずに、これらの入力特徴を直接マッピングして出力要素を予測し、構成的推論を実行しているかのように見せかける可能性が高いことが示唆される
        - [意見] うーん予防線が多い。結論を急ぎすぎている気がする
  - Transformers はマルチステップの構成的推論を線形サブグラフマッチングに変換しているのでは？
    - モチベーション：
      - 未知のテストデータに対するモデルの正しい予測が、基礎となるアルゴリズムの学習によるものなのか、類似の訓練例にふれることで説明可能なものなのかを調査
    - 仮説：
      - Transformers は単純な記憶だけでなく、compositional tasks を解くためにパターンマッチングに大きく依存しているのではないか？
    - 実験：
      - 正しく予測された例と間違って予測された例の両方について、ある例を解くのに必要な部分的な計算が訓練データに現れる平均頻度を計算
        - 平均頻度が高い（＝訓練データにたくさん例が入っている）計算の正解率が高く、平均頻度が低い（＝訓練データに例が少ない）計算の正解率が低かったら、パターンマッチしていることになるよね
    - 結果：
      - 正しく予測されたテスト例のほうが、正しく予測されなかったテスト例よりも、例を解くのに必要な部分的な計算（完全計算部分グラフ）の出現頻度が有意に高い
      - Transformers に一般的な推論能力があるのではなく、パターンマッチングが正しいモデル出力の原因である可能性をを示唆
        - パターンマッチングは、タスクの構成的複雑度が低い場合は効率的だが、複雑度が高くなると効率が低下する

  - 推論の深さが異なる場合の、Transformersの失敗の調査
    - モチベーション：
      - Transformersがとこで失敗するかをより明確に理解したい
    - 仮説：
      - 推論の深さが深くなるとどこかで失敗することが多い
      - どこで失敗するかに、Transformersのエラーの方向性が見えてくるかも
    - 実験：
      - グランドトゥルースの計算グラフとモデルが生成した計算グラフを比較
      - 計算グラフの各ノード$v$を次の4つのカテゴリに分類
        - 完全に正解：$v$が正しい親ノード正しい計算から導かれる正しい値である場合
        - 局所エラー：親ノードが正しい値を持つが、$v$が正しくない計算から派生する場合のエラー
        - 伝播エラー：$v$は正しい計算から派生するが親ノードの一部が正しくない値をもつ場合のエラー
        - 復元エラー：$v$が正しい値を持つが、正しくない計算から派生した場合のエラー
    - 結果：
      - グラフ層が増加する（＝問題を解くのに必要な推論のステップ数が増える）につれて、正解率は急激に減少
      - 伝播エラーの比率が局所エラーの比率よりも高い
        - 言い換える（↓あってるか？）
          - 伝播エラー：計算方法は正しいけど、ひっぱってくる変数がミスってる
          - 局所エラー：ひっぱってくる変数は正しいけど、計算方法がミスってる
      - DP, パズルでは復元エラーの比率が高い
      - ここからわかること
        - モデルが単一ステップの推論を正しく実行できる
        - 全体として正しい推論のためにこれらのステップを計画し、構成することはできない
        - DP, パズルでは、誤った計算をするにもかかわらず正しい出力が生成されていることが多い
          - 訓練データからよくある（入力、出力）のペアを丸暗記してるっぽい
  - Transformers が複雑な多段階の推論タスクを扱う上での理論的な限界
    - *詳細はスキップ*
      - Transformers が推論において$n$個の独立したfunctionの適用するアルゴリズムを推定するシナリオで、$n$が増えるほど推定の誤差が蓄積してきて、ミスがミスを呼び正しい推論に回復できず急速に失敗しまくることを定式化
      - [意見] 仮定の上のモデルの推定の推定の話。
        - 復元エラーがおきた場合の扱いどうするんだ？
  - Discussion
    - Collapsed Compositionality and Robustness Implications 
      - Transformers は多段階の構成的な演算を必要とするタスクに弱い
      - 解ける場合もあるが、その場合は問題の解法にショートカットがあり、推論の深さが実質的に縮まっていることで、解けているように見えているだけかも（近道のパターンマッチによって許容可能な解が生成できているだけ）
    - Theoretical Findings and their Empirical Implications 
      - 4節で示した証明では、合理的な仮定（？）の下で、compositonal tasks では誤った予測の確率が指数関数的に1に収束することを示した
        - 証明は、一般的な自己回帰言語モデルに適用可能
      - Transformers は「推論の深さ」が小さい問題を解くときには採用してもいいかもね
      - Transformersは評価メトリクスがある程度観葉な場合には採用してもいいかもね
        - ↑それ「Transfomersは」でいいか？手法の話か？問題の話じゃないか？
      - Transformersを補助するモジュールを用いて、Transformersに補助するモジュールの使い方を改善させることで、できること増やせばいいんじゃない（筆者曰く論文提案してますとのこと）


## 批評
- 質問-回答形式と質問-スクラッチパッド形式の差異がわからなかった
  - 形式を変えるだけで、LLM内での扱い方が変わるのか？
  - step-by-step形式で出力する形でもcompositional tasksは解けませんでしたというだけ？
- 8つのLLMの混合モデルであるGPT-4なら、compositional tasks特化のモデルがあれば解けそう？
  - このストーリーだと、「OpenAIの作ったモデルのリバースエンジニアリング」になっている
  - これまでのLLMだと解くのが難しいcompositional tasksに対応するために、既存のモデルにこの機能を追加する、というストーリーだとまだ自然
    - 多数の変数を保持しとく外部ブロックを参照できるだけで推論の深さと幅由来の困難さは克服できそうじゃない？
    - 6節のRelated Worksにあがってたけど、まだまだ銀の弾丸ではないみたい
      - 外部ブロックを使っても、問題がTransformersが扱える推論深さ・幅にならないってことかな？
- 相対情報利得によって、表層パターンが予測できるってのもすっと入ってこない
  - 問題によって偽のパターンが出現しうるってのはわかるけど、LLM内で偽のパターンをなぞるショートカットが学習されやすいかもっていうのは早くない？
  - Transformersの特徴マップが偽のパターンに発火しているか・その発火が後段にも影響するか・偽のパターンの学習を阻害すれば問題が回避されるかを確認していく必要がありますよね？
  - 8節のLimitationsにあったけど、予測時にも出るが注目する正確なトークンを調べることができないから、代わりにモデル生成とその前のコンテキストとの間の相関関係使いましただって
    - OpenAIのみぞ知るって話ですわね


## 次に読むべき論文
- [Show Your Work: Scratchpads for Intermediate Computation with Language Models](https://arxiv.org/abs/2112.00114)
  - スクラッチパッド形式を提案している論文
  - 思考の過程を擬似アルゴリズムみたいな形で表現するものらしい
  - Figureだけちらっと見たけど、パラメータが増えると正解率が100%ぐらいになってた
    - それ過学習してるだろ…