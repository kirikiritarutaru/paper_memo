# HiPPO: Recurrent Memory with Optimal Polynomial Projections

## 論文について (掲載ジャーナルなど)
- [Gu, Albert, et al. "Hippo: Recurrent memory with optimal polynomial projections." Advances in Neural Information Processing Systems 33 (2020): 1474-1487.](https://proceedings.neurips.cc/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf)

## 概要
- 逐次的に入ってくる時系列データから学習することを考える
    - 中心的な課題は、「多くのデータが処理されるにつれて、どんどん増えてくる履歴を段階的に表現していくこと」である

- 本論文では、多項式関数の基底へ射影することにより、連続信号や離散時系列を**オンライン**で圧縮する一般的なフレームワーク（HiPPO)を提案
    - 過去の各時刻ステップの重要度を指定する尺度が与えられると、HiPPOは自然なオンライン関数近似問題に対する最適解を生成する

- 提案する新しい記憶更新機構（HiPPO-Legs）は以下の理論的な利点がある
    - 時間スケールに対するロバスト性
        - DNNやLSTMは同じ時間スケールのデータでないと精度がでないが、HiPPOは異なる時間スケールのデータでも性能が落ちない

    - 高速な記憶の更新
    - 有界なGradients


## 問題設定と解決したこと
- 逐次的に入ってくるデータのモデリングと学習は、以下のタスクの基礎となり、基本的な問題
    - 言語モデリング、音声認識、映像処理、強化学習

- 長期的かつ複雑な時間依存性をモデル化する上で、「記憶＝以前の時間ステップからの情報を保存し、取り込むこと」が重要
    - より多くのデータが逐次的に入ってきたときを考えると、記憶はオンラインで更新される必要がある
    - 限られたストレージを用いて、蓄積されるhistory全体の表現を学習しなければならない

- 上記の問題に対する既存の手法として、RNNがあげられる
    - どんな方法？
        - 多くの情報を取り込みながら時間と共に変化していく状態をモデル化するアプローチ

    - RNNの課題は？
        - メモリバク食い
        - gradientsの消失

    - RNNの課題を克服するヒューリスティックな方法として、以下が挙げられている
        - LSTM、GPU、ユーリエ再帰ユニット、ルジャンドルメモリユニット（LMU）

    - それでもまだ残る課題
        - gradients の消失
            - 数万ステップもあるような、とても長いデータだと記憶が保持できない
        - シーケンス長や時間スケールに対する事前に仮定を置いている
            - シーケンス長、時間スケールが異なるデータには適用できない
        - 「長期依存性をどの程度うまくとらえることができるか」についての理論的保証が無い

- 本論文で解決したこと
    1.  記憶に扱う方法の理論的な定式化
        -   既存手法の統一的な見解を示した

    2.  時間スケールへの事前の仮定なく、任意のシーケンス長のデータを扱うことができる手法の提案
    3.  記憶を扱うことに対する厳密な理論的証明


## 何をどう使ったのか
- 提案手法 **HiPPO** (high-order polynomial projection operators) 
    - 任意の関数を与えられた尺度に関して直交多項式空間に射影する演算子
        - 尺度：過去における各時刻のデータの重要度

- 入力関数$f(t) : \mathbb{R}_+ \rightarrow \mathbb{R}$が与えられたとき、逐次的に入ってくる入力を理解し、将来の予測を行うために、時刻$t\ge 0$ ごとに累積history $f_{\le t}:= f(x)|_{x\le t}$ を操作することが必要
    - 関数空間は非常に大きいので、historyを完全に記憶することができない→圧縮する
    - historyを有界次元の部分空間に射影する
    - しかも、圧縮したhistory の表現をオンラインで更新していく

- HiPPOの数学面の構成要素
    1.  測度に関する関数近似
        -   近似の質を評価するために、関数空間における距離を定義する必要がある
        -   $[0,\infty)$上の確率測度を指定することで、（関数と関数の内積を定義し、）2乗可積分関数の空間＝ヒルベルト空間を用意する
            -   内積から誘導されるノルムも自然に定まる

    2.  多項式基底展開
        -   関数空間の任意の$N$次元部分空間$\mathcal{g}$は、近似のための適切な候補
        -   パラメータ$N$は近似の次数＝圧縮の大きさに相当し、投影されたhistoryは$\mathcal{g}$の任意の基底における展開の$N$個の係数で表すことができる
        -   本論文では、$\mathcal{g}$が$N$未満の次数の多項式の集合であるように、自然基底として多項式を用いる

    3.  オンライン近似
        -   時間$t$ごとに$f_{\le t}$を近似することに関心があるので、測度も時間によって変化させる
            -   すべての$t$について、$\mu^{(t)}$を$(-\infty,t]$上の測度とする
                -   なんで$(-\infty, t]$？→$f_{\le t}$は時間$t$までしか定義されないので

        -   $\|f_{\le t}-g^{(t)} \|_{L_2 (\mu^{(t)})}$を最小化する$g^{(t)}\in\mathcal{G}$を求める
            -   直感的な解釈
                -   測度$\mu$が入力されるデータのそれぞれの部分の重要度をコントロール
                -   基底は許容される近似を定義する

        -   解きたい課題としては、
            -   $\mu^{(t)}$が与えられたときにいかにして閉じられた形で、（与えられた関数と近似する関数の距離を最小化する）最適化問題をとくか
            -   $t\rightarrow \infty$のときに、基底の係数をオンラインで維持するか


## 主張の有効性の検証方法
- 

## 批評
- 

## 次に読むべき論文
- 
