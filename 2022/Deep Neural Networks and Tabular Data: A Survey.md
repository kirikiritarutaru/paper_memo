# Deep Neural Networks and Tabular Data: A Survey

## 論文について (掲載ジャーナルなど)
- [Borisov, V., Leemann, T., Seßler, K.,  Haug, J., Pawelczyk, M., & Kasneci, G. (2021). Deep neural networks  and tabular data: A survey. *arXiv preprint arXiv:2110.01889*.](https://arxiv.org/pdf/2110.01889.pdf)

## 概要
- 表敬式のデータに対する最先端の深層学習手法の概要を説明
    - データ変換・特殊なアーキテクチャ・正則化モデルのグループに分類
    - 各グループの主要なアプローチの概要を述べる

- 従来の機械学習手法と深層学習アプローチの実証的な比較
    - DBDTに基づくアルゴリズムの性能は、深層学習アプローチを上回る結果に


## 問題設定と解決したこと
- 画像、音声、テキストデータでは、DNNは高性能
- しかし、**”表形式のデータ”**ではまだまだ
    - なぜ？
        - 密な数値的特徴と疎なカテゴリ特徴をもつ
        - 特徴間の相関は、画像や音声データの空間的・意味的関係よりも弱い
            - 空間情報に頼らずに相関を発見し利用する必要がある
- 表形式のデータは落とし穴多数
    - ノイズ、不確かさ、異なる属性タイプ、値の範囲、値の利用不可能性
        - DNNは柔軟だし効率的で反復的な学習が可能だから対処できるかも？
- 表形式のデータ、特に異種データ（Heterogeneous data）で重要なタスク
    1.  異種データの生成タスク
        - 異種の表形式のデータは収集に多大なコストと時間がかかる

            - 生成して水増ししたらいいんじゃね？
                - 表データ生成は”闇”なので、非常に複雑
                - 最先端のアプローチについてはセクションVで議論

    2.  表形式データに対するDNNの解釈タスク
        - 一般的アプローチは、グループがハイライトされること（saliency マップみたいに）
        - *変数関係の強調*←これが特に重要
            - アテンションメカニズムによって、変数関係を強調（アテンションマップの可視化）
- 本論文における議論の概要
    1.  表形式のデータに対するDNNの既存の論文のレビュー
    2.  異種表形式データの分類と回帰タスクのためのアプローチを分類
    3.  表形式データの生成に向けた技術の現状と展望の提示
    4.  表形式データのためのDNNの既存の説明アプローチの概要（DNNの解釈性）
    5.  複数の実世界の異種表形式データセットにおける従来の機械学習手法とDNNモデルの実証的比較
    6.  表形式データに対するDNNの成功が限定的である主な理由についての考察
    7.  表形式データのDNNに関連する未解決課題のリスト




## 何をどう使ったのか
- 深層学習モデルの中だとTabNetが特に有名
    - TabNetによって、Attentionに基づく手法と自己教師付き学習手法によって、深層学習モデルが表形式のデータに適用されはじめた
    - Transformerをベースとした手法は表形式のデータセットに対して人気になってきている（2022現在）



- 表形式のデータに対しては、DNNは（普通）線形や木ベースといった伝統的な機械学習手法よりも精度が悪くなりがち
    - ↑原因はよくわからないことが多い
    - 考えられる理由
        1.  不適切な学習データ
            -   データの質が悪いケースのこと
                -   欠損値、極端なデータ、誤ったデータ、矛盾したデータ
                -   データから生成される高次元の特徴ベクトルに比べてデータセットのサイズが小さい問題
                -   データの収集にコストがかかるため、クラスが不均衡になる問題

        2.  空間依存性の欠落または、複雑で不規則な空間依存性
            -   表形式のデータセット自体がもつ難しさのこと
                -   変数間に空間的相関がないことが多い
                -   もしくは、特徴間の依存関係が複雑で不規則なことが多い

            -   畳み込みニューラルネットワークは均質なデータ（特に画像データ）のための構造（＝データに置く前提）をもつので、表形式のデータセットのモデル化には適さないことが多い

        3.  広範な前処理
            -   カテゴリ特徴を扱うことの難しさのこと
                -   カテゴリ特徴はほとんどの場合、（最初のステップとして）単純な one-hotベクトルへの変換や 順序エンコーディングをかけて数値表現に変換する
                -   しかし！カテゴリ特徴は非常にまばらなため、↓につながる可能性がある
                    -   one-hotベクトルへの変換した場合：非常にまばらな特徴行列になる
                    -   順序エンコーディングをかけた場合：非順序値の synthetic alignment

            -   DNNの前処理手法を適用すると、元のデータに関する情報を失うことが多く、予測性能の低下につながることもあるやで！

        4.  モデルのSensitivity
            -   DNNの頑健性の低さのこと
                -   DNNは入力データの小さな摂動に対して弱い（カテゴリ特徴の小さな変化が、予測に大きな影響を与える）
                -   決定木系のアルゴリズムは特徴と閾値を選択し、残りを無視するので、例外的に摂動を処理することができる

            -   表形式のデータにDNNを適用する時は、強い正則化をかけると良さげ（セクションⅣ-Cで議論する）
            -   DNNはハイパラが非常に多いので、チューニングに時間がめっちゃかかるし、ハイパラの選択に鋭敏な傾向がある←使いづらい



### 表形式のデータに対する深層学習のアプローチ

- 大別して、3つにわけられる
    1.  データ変換手法
        -   （１）1次元エンコーディング
            -   DNNは実数ベクトルしか入力として受け付けない→DNNモデルに適した形でカテゴリ変数をエンコードしなければ！
            -   各特徴を個別に変換
                -   orginal encoding
                    -   *特徴内*のカテゴリ変数の項目数に対応して連番を振る変換
                -   label encoding
                    -   *ターゲット*のカテゴリの項目数に対応して連番を振る変換
                    -   上２つは順序付けされていないカテゴリに人工的な順序を導入してしまう問題がある
                -   one hot encoding
                    -   観測されたカテゴリに対応する列のみ1が割り当てられる変換
                    -   多様なカテゴリがある場合オワオワリ
                -   Binary encoding
                    -   連番を振るのではなく、数値をバイナリーフォーマットで表現する変換
                -   Leave-one-out encoding
                    -   すべてのカテゴリがそのカテゴリのターゲット変数の平均値で置き換えられる変換（ただし、ターゲット変数の平均値を計算するときに現在の行を除外）←過剰適合の回避のため
                -   hash-based encoding
                    -   すべてのカテゴリをハッシュ関数によって固定サイズの値に置き換える変換
        -   （２）多次元エンコーディング
            -   データレコード全体を別の表現にマッピング
            -   VIMEアプローチ
                -   表形式データのための自己・半教師付きDNNフレームワークを提案
                -   2つのプレテキストタスクを用いて自己教師ありのアプローチでエンコーダを学習
                    -   サンプルのどの値が破損しているか判断
                    -   破損している箇所の復元
                -   エンコーダはカテゴリ的（または数値的）特徴を、新しい均質で情報量の多い表現に変換する
                    -   変化された特徴ベクトルは予測モデルへの入力として用いられる
            -   SuperTML method
                -   表形式データ→視覚データ形式（2次元行列）に変換し、CNNを用いてコンピュータビジョン的に解く
                -   IGTDフレームワーク
                    -   SuperTMLと同様のアイデア
                    -   画像への変換は表形式データの特徴距離のランキングと生成画像の画素距離のランキングを最小化することで最適化
                        -   特徴の関係が強い表形式データでは比較的良好
                        -   特徴が独立している場合や特徴の類似性で関係が特徴付けられない場合には失敗することも
        -   Pros：カテゴリや数値データを変換し、DNNモデルが情報をより良く抽出できる
        -   Cons：変換ステップをかますので、前処理の時間が増加（データセットのサイズが大きくなるとやばお）

        

    2.  特殊なアーキテクチャ

        -   ハイブリッドモデル
            -   古典的な機械学習アプローチ（決定木など）とDNNを融合させたもの
            -   表形式データに対するDNNのアプローチのほとんどはハイブリッドモデル
                -   Fully differentiable Models （完全微分可能なモデル）
                    -   勾配降下オプティマイザを用いた学習と推論のためのE2EのDNNを可能にする→GPUやTPUによる学習・推論の高速化ができるということ
                    -   NODE フレームワーク[6]
                        -   CatBoostフレームワークの一般化
                        -   微分可能なoblivious 決定木とDNNのアンサンブル
                            -   微分可能にするためにentmax 変換とソフトスプリットを利用
                        -   GBDTモデルよりも高性能（と報告されている）
                    -   DNNを解釈しやすくするために、ソフト決定木（SDT）による別モデル[81]
                        -   第一段階：DNNを学習→DNNの出力とグランドトゥルースのラベルを混合
                        -   第二段階：SDTモデルで上の混合物を学習
                            -   DNNをSDTで蒸留ってことやな
                            -   精度若干↓、解釈性↑、効率的な推論↑（らしい）
                    -   ソフト決定木回帰器（SDTR）[83]
                        -   2値決定を模倣しようとするニューラルネットワーク
                        -   低メモリ環境下では良い結果
                            -   ↑あやしすぎる
                    -   Net-DNF
                        -   GBDTのアルゴリズムの特性を模倣することができるニューラルネットワークのアーキテクチャ設計
                        -   使用されたデータセットが数値特徴がほとんどで、バイナリ特徴をほとんど含まないため、high-cardinalityカテゴリデータをどう扱うかについて言及されていない
                            -   ↑あやしすぎる〜〜〜〜〜〜！！！！！！
                    -   線形モデルとwide&DeepなDNNのハイブリッド[86]
                        -   DNNを使用して非線形な方法で特徴を組み合わせ、得られた組み合わせを線形モデルへ入力する
                        -   解釈可能性を提供しながら、単純なモデルを強化
                    -   DeepFM[88]
                        -   手作業による特徴変換（特徴量エンジニアリング）を、学習されたFactorization Machines (FMs)で置き換える
                            -   FMは、高次元の疎なデータ内の特徴間の相互作用を効率的にとらえるために設計された「線形モデルの拡張」
                    -   Network-on-Network (NON)[89]
                        -   特徴内の情報を効率的に捉えることに重点をおいた表形式データの分類モデル
                        -   3つのコンポーネントから構成
                            -   列固有の情報を捉えるために、列ごとに1つのユニークなDNNからなるfield-wise network
                            -   データセットに基づいて最適な演算を選択するacross-field-network
                            -   非線形性を考慮して選択した演算を接続するoperation fusion netowrk
                        -   特定のデータに最適な演算を選択するため、他のDNNモデルと比較して高性能
                        -   ただし、**決定木と性能比較していない**&計算量やばお
                            -   ↑あやしすぎる〜〜〜〜〜〜〜〜〜〜〜〜〜〜〜〜！！！！！！！
                -   Partly differentiable Models
                    -   DeepGBM
                        -   DNNの柔軟性とGBDTの前処理能力をくみあわせたもの
                        -   コンポーネントは2つ
                            -   CatNN
                                -   疎なカテゴリ特徴を扱うことに特化
                                    -   カテゴリデータを順序エンコーディングによって変換
                                    -   数値特徴は離散化
                            -   GBDT2NN
                                -   密な数値特徴を扱うことに特化
                                    -   決定木の葉のインデックスにアクセスし、GBDTに基づくモデルからデータ・セットに関する知識を抽出	
                                        -   決定木の葉に基づく埋め込みってこと
                        -   Pros：GBDTより高い性能
                        -   Cons：葉の指標は直接比較できないため、メタカテゴリ特徴とみなされる・欠損値、数値特徴のスケーリングの違い、ノイズなど他のデータに関する問題が、モデルの予測にどのような影響を与えるかは不明
                            -   ↑ちょっとわからんな…
                    -   TabNN
                        -   表現力豊かな特徴の組み合わせを明示的に活用する＋モデルの複雑さを軽減する工夫を加えたアーキテクチャ
                        -   GBDTから知識を抽出し、特徴グループを取得しクラスタリング、それらの特徴の組み合わせに基づきDNNを構築
                        -   Cons：ネットワークの構築に大規模な計算ステップが必要、構築の課題を考慮し、TabNNの実装が提供されていないので、ネットワークの実用的な使用は限定的
                            -   ↑人間がゴリゴリ工夫する作業の一部を計算力で回避しています＋まだ人間がゴリゴリ工夫する部分が残っているので一部のデータ・セットにしか適用できませんってコト…!?
                            -   あやしい〜〜〜〜〜〜！！！！
                    -   BGNN (Boosted Graph Neural Network)
                        -   グラフニューラルネットワークを用いて決定木からトポロジー情報を抽出し利用
                            -   決定木って有向グラフの形式をもつらしい
                        -   予測性能と学習時間の点で他のアルゴリズムよりも優れている（と報告されている）
        -   Transformerベースのモデル
            -   Attention構造に依存したもの
            -   TabNet
                -   表形式データに対する最初のTransformerに基づくモデル
                -   複数のサブネットワークから構成される
            -   TabTransformer
                -   Attention機構に基づく変換器を用いて、カテゴリ的特徴をcontextual埋め込みにマッピング
                    -   この埋め込みは、欠損やノイズの多いデータにたいしてより頑健かつ解釈可能
                -   埋め込んだカテゴリ特徴量＋数値特徴量を単純なMLPに入力して学習
            -   ARM-net [100]
                -   入力特徴を指数空間に変換し、各特徴の組み合わせに対して適応的に相互作用順序と相互作用重みを決定することにより、結合特徴と特徴相互作用を選択的かつ動的にモデル化
                    -   ↑わからん
            -   SAINT (Self-Attention and Intersample Attention Transformer) [9]
                -   自己注意機構と複数行にわたるサンプル間の注意を組み合わせたハイブリッドAttentionアプローチ
                -   欠損やノイズの多いデータを扱う場合、上記メカニズムにより、モデルは類似のサンプルから対応する情報を借りることができる
                -   サンプルをAugmentするために、入力空間ではCutMixを使用し、埋め込み空間ではmixupを使用する

        

    3.  正則化モデル

        -   主に、特殊な目的の損失関数の形で実装されているアプローチ
            -   表形式のデータに対するDNNモデルのパフォーマンスがそんなに高くないのは、極端な非線形性とモデルの複雑さが原因ではないかという推測からこのアプローチがとられている。
        -   正則化学習ネットワーク（RLN）
            -   DNNの各重みに学習可能な正則化係数を適用して、モデルの柔軟性を下げる
            -   RLNの論文では、既存手法との比較に数値データを含む1種類のデータセットしか使用されておらず、カテゴリデータの問題には触れていない
                -   ↑あやしいいいいいいいいいいいい！！！！駄目では？？？？
        -   [8]の論文では、DNNが適切に正規化されれば、表形式データにおいて単純なMLPが最先端のアルゴリズムを上回ると主張されている
            -   13個の異なる正則化手法の「カクテル」を提案
            -   データセットごとの正則化とハイパーパラメータの最適化に膨大な時間がかかるで！
                -   これある意味データ・セットごとに過剰適合/リークでは？？？？？？？



### 表形式のデータで重要なタスク

- 表形式データの生成タスク

    - 特に関心の高いタスクは以下の3つ
        - 欠損値埋め合わせ
        - リバランスによるデータオーグメンテーション
        - プライバシーを考慮した機械学習
            - 生成したデータを用いてプライバシーに関する懸念を克服
    - 離散的特徴と連続的特徴の混合されていることと、それらの異なる値分布を管理する必要があり困難なタスク
    - アプローチ
        - 古典的手法
            - Copulas
            - Bayesian Networks
                - 近似にもとづく手法[126]が特に有名
        - DNN系
            - GANベースのもの
                - Wasserstein GAN (WGAN)
                - WGAN with gradient penalty (WGANGP)
                - Crame'r GAN
                - Boundary seeking GAN
                    - 離散データを念頭に置いて設計
                - VeeGAN
                    - 表形式データによく利用される
                - MedGAN
                    - 患者記録を生成するためのDNNモデル
                        - ドメイン特化やなぁ
                    - すべての特徴が離散的特徴
                - table-GAN
                    - 表形式データにCNNを適応させたもの
                    - 畳み込みのような画像に用いられる帰納的バイアスが表形式データにどの程度適しているかは不明
                        - ↑おいおい
                - CTGAN
                    - 1つのデータポイントの特徴間の相関に注目したアプローチ
                        - 各カラムに対して適切な条件付き分布を学習させるようにしたもの
                    - 2021年の最先端
            - VAE
                - TVAE (Tabluar Variational Autoencoder)
                    - 2021年の最先端
    - 生成されたデータの品質を評価する尺度
        - 代理の分類タスクを定義し、それに対する1つのモデルを実際のトレーニングセットで、もう一つを人工的に生成したデータセットで訓練→高い能力を持つジェネレーターを使えば、実データのテストセットに対する人工データモデルの予測性能は、実データに対応するモデルとほぼ同等になるはず！
            - 実際のトレーニングセットと同じものを出力するジェネレーターだったらどうするん（トレーニングセットに超絶過剰適合しているようなジェネレーター）
        - misc
            - 任意の特徴をラベルとして使用し、予測
            - 特徴量ごとにモデル化されば分布の可視化
                - 累積分布関数
                - 散布図で期待値を比較
        - 定量的なアプローチ
            - Kolmogorov-Smirnov検定などの統計的検定を使用して分布の違いを評価したり
            - 対数尤度の観点から、生成されたデータセットとグランドトゥルースとを比較
                - 決定的な方法はないってコト…!?

- 表形式データの解釈タスク

  1.  特徴の強調による説明
      -   Local input attribution techniques
          -   モデルに利用可能なすべての入力が、ある予測に到達するためにどのように利用されているかを理解することを目的とする
  
      -   ネットワークの設計によって説明可能な分類モデルを構築する
          -   局所的に線形であることを強制したモデルの構築
          -   モデルのパラメータが既知でアクセス可能な場合、パラメータを使用してモデルの説明する
              -   勾配ベースの手法
  
          -   モデルのパラメータが未知でアクセスできない場合、不可知論的アプローチが有用（？？？）
              -   サロゲートモデルを適用し、モデルの挙動を局所的に説明
              -   [109], [157]-[160]←**ここらへんおもしろいかも**
  
      -   [XAI-Bench](https://github.com/abacusai/xai-bench)
          -   合成データを使用して特徴アトリビューションテクニック
          -   モデル内の各特徴が各インスタンスの予測にどの程度影響を及ぼしたかを示す手法が多く実装されたライブラリ
  
  2.  反実仮想的説明
      -   出力がエンドユーザによって有利になるようにDNNへの入力への介入を提案することが目的（？？）
      -   Independence-based approaches
          -   予測モデルの入力特徴が独立であると仮定される独立ベースの手法
          -   実現可能性制約の存在下で再帰性を生成するために組み合わせソルバーを使用
          -   低コストの反事実的説明を見つけるために勾配ベースの最適化を展開
              -   ？？？やばい意味不明すぎる。ここ長くいるだけ損。論文味見するほうが有意義
  
      -   dependenced-based approaches
          -   文章から読み取れん…
          -   生成モデルつかう？もうだめだ…
  
      -   [CARLA](https://github.com/carla-recourse/CARLA)
          -   反実仮想的説明のベンチマークライブラリ
  
  
    


## 主張の有効性の検証方法
- 

## 批評
- アプローチのいくつかわからんかったな…
    - entmax transorm, soft split

- サーベイだからか手法の列挙になってしまった。
- 分類されているけど、ぶっちゃけ後から参照しにくいまとめかたになってしまった。もうちょっと読み飛ばせばよかった反省
- 表形式のためのハイブリッドなDNNモデルあやしいの多すぎやろ…

## 次に読むべき論文
- [R. Shwartz-Ziv and A. Armon, “Tabular Data: Deep Learning is Not All You Need,” 2021.](https://arxiv.org/pdf/2106.03253.pdf)
    - 表形式データ用の深層学習モデルがいくつか提案されて、XGBoostよりも高性能とかいってるけど本当？
    - いろんなデータセットで比較してみました！→XGBoostのほうが性能いいし、チューニングも簡単だということがわかったで！
        - 深層学習モデルとXGBoostのアンサンブルならワンチャンあるかもな！
- [M. Sahakyan, Z. Aung, and T. Rahwan, “Explainable artificial intelligence for tabular data: A survey,” IEEE Access, 2021.](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9551946)
    - 表形式データにおけるXAIのサーベイ論文
