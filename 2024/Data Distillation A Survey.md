# Data Distillation: A Survey

## 論文について (掲載ジャーナルなど)
- [Noveen Sachdeva, Julian McAuley](https://arxiv.org/abs/2301.04272)

## 概要
- 大規模なデータセットで膨大なパラメータのネットワークをtrainingすると、以下の問題が！
    1.  モデルの学習時間が長い
    2.  研究のイテレーションが遅くなる
    3.  計算資源食いまくる→電気つかいまくる→環境に悪い
- モデルの学習・推論・*アーキテクチャ探索*などにおいて、効果的な代替品あると嬉しいよね
- 本サーベイでは、データ蒸留のための諸々のアプローチの取り上げと現在の課題、今後の研究の方向性についてまとめたよ

## 問題設定と解決したこと
- 

## 何をどう使ったのか
- 

## 主張の有効性の検証方法
- 

## 批評
- 学習済みモデル配ったほうが楽だし早くない？
    - データちっさくしても結局学習して同じような重み目指すんでしょ？

- モデル蒸留のほうがまだわかる
- ちっさくしたデータセットでもとのデータセットと同じようなNASが達成できるか怪しくない？
    - わっかんねーーーーー


## 次に読むべき論文
- 
